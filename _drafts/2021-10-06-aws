2021-10-06 AWS 강의 1일차

Kubernetes : 컨테이너 관리 도구
VM? App? Image?

왜 App은 컨테이너에 올려서 배포하는걸까? 목적이 뭘까?
  - 확장성
  - 관리용이성
  - 리소스(메모리) 관리
  - 마이크로 아키텍처 MSA

App을 컨테이너화 하는 대표적인 이유!
  - VM 보다 I/O에 대한 성능이 좋아짐?
  - 경량 VM이라고 볼 수 있다.
  
  Q1) 컨테이너가 On-Prem에 비해서 Kernel을 가지고 있지 않아서(공유해서) 경량이라는 말씀으로 이해해도 될까요?
  Q2) 경량이라는건 이해가 되는데, 경험적으로 I/O가 성능이 좋아진다는 경험은 못한 것 같은데,
      I/O 성능이 좋아지는 것이 어떤 예가 있을까요?
      App의 file, network i/o 성능이 On-Prem OS 상태일때 보다 좋아진다고 봐야 하나요?
      
  A) 아키텍처를 그려보면 보인다. 보통 On-Prem 위에 VM을 올려서 App을 배포하는데,
     이때는 On-Prem에도 kernel, VM에도 kernel이 있게 된다.
  
  
경량화 OS가 목적인 과제가 있는데, 관련해서 궁금한 점이 있습니다.
Q1) 경량화 OS가 목적이다 보니, Pod 하나 위에 컨테이너를 하나 올려서 서비스로 제공합니다.
    사실 유저에게 VM과 같은 환경을 컨테이너로 안정적으로 제공하고 싶었습니다.
    가장 처음 만난 문제가 Pod eviction 문제였습니다.
    간단하게 생각했을때, Pod eviction이 일어나지 않도록
    (1) Pod 리소스를 충분히 제공하는 것
    (2) eviction이 일어나지 않도록 configure를 수정하는 것을 검토했었습니다.
    이런 사례에서 정답이라고 할만한 방법이 없을까요?
    OS컨테이너가 좋지 못한 선택인가 싶어서요...
Q2) 경량화 OS(VM)을 목적으로 하는 과제가 현장에 사례가 많이 있나요?

A) VDI를 컨테이너로 가져가려는 시도들도 많이 있다.


네트워크 로직은 별도로 빼내서 만들고 관리하자. - 서비스 매시
엔터프라이즈 환경이 컨테이너들의 집합으로 커지면 결국 서비스 매시하고 연결될 수 밖에 없다.
이런 것들이 갈수록 생태계를 만들어 가고 있다.

k8s는 자동화 환경이 중요한 목적이기 때문에 GUI 콘솔 도구가 별로 없다.
CI/CD의 자동화가 큰 목적이다.

선언형
  - 개발에 있어서 코드 작성이 명령어 방식과 선언형 방식이 있는데, 어떤쪽이 자동화에 더 유리한가.
  - 선언형이 더 유리하다. yaml, json등을 사용하게 되는 배경.

k8s 내부 구조
  - 컨트롤 플레인 : 적절한 라우팅에 대한 결정 및 권한
  - 데이터 플레인 : 라우터를 통해 트래픽 전달

여러대의 호스트가 필요해지고 -> 클러스터 : 노드의 그룹

클러스터
  - 예전에는 master / slave
  - 최근에는 manager / worker 로 변경
  - 다른용어중에 control plane / data plane(또는 node cluster)
  
  Client -> req -> API Server -> Scheduler
    API Server : node 중에 어디에 배포할 것인가? 판단
                 이 판단을 하는 녀석 Scheduler

  이 요청을 받아서 처리하는 녀석이 kubelet
  kubelet이 일을 하고 결과에 문제가 없으면 -> API Server
  
  현상 : current
  사용자 요청 : desired (nginx를 2개 띄워라 같은 ...)
  
  Controller가 desired와 current가 맞는지(sync) 체크를 한다.
  
  그리고 데이터는 etcd에 저장
  --
  etcd, API Server, Scheduler, Controller 를 묶어서 control plane 이라고 부른다.
  
  --
  node cluster쪽은
  kubelet
  kube-proxy : 컨테이너의 통신을 책임진다
    - 최근에는 kube-proxy는 kernel 레벨의 iptables를 이용한다.
  
  --
  그리고 현업에서는 여기에 수 많은 plugin이 들어간다.
  
 --
 시스템이 커지면 위 등장인물들중 누가 워크로드가 가장 커질까?
 모든 통신은 API Server를 거쳐야 한다. 그래서 API Server가 워크로드가 가장 커진다.
 
 --
 EKS는 control plane은 보이지 않는다. 고객은 node cluster만 볼 수 있다.
 즉, EKS는 control plane을 관리해주는 관리형 서비스다.
 
 파게이트는(fargate)로 구성하면 Data plane도 보이지 않는다.
 이 경우는 Data plane도 EKS가 관리해 준다.
 
 --
 API Server는 Endpoint가 중요하다.
 API server는 2개의 Endpoint를 가지고 있다.
 하나는 내부를 위해서 하나는 외부와 연결하기 위해서.
 
 $ k cluster-info
Kubernetes master is running at https://8D3545D540B0C7E179498F5082A8BB1F.yl4.ap-northeast-2.eks.amazonaws.com
CoreDNS is running at https://8D3545D540B0C7E179498F5082A8BB1F.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://8D3545D540B0C7E179498F5082A8BB1F.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

여기 보이는 https 주소가 외부 endpoint

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
LGESCSICBLD37V[ /home/dongyoung.yoon/remove-cockpit-worker]$ k get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   172.20.0.1   <none>        443/TCP   223d
 
여기 보이는 ip가 내부 endpoint

-- 
Pod에 대해서
  - ReplicaSet이 Pod와 묶이면, Controller의 제어가 들어가게 됨
  - Controller 제어가 들어가면 Managed Pod
  - 그렇지 않은 Pod는 Unmanaged Pod
  - 보통 ReplicaSet을 직접 사용하지 않고, Deployment를 활용해서 ReplicaSet을 핸들링 한다.
  - Deployment > ReplicaSet > Pod 로 제어된다.
  - 보통 비동기 병렬처리로 job을 핸들링 한다고 보면 된다.
    수많은 App. 여러개를 병렬로 job을 실행한다고 보면 된다.
  - StatefulSet > Pod에 영향
  - Volume은 PVC, PV, Storage Class 등
  - HPA, VPA
  - ConfigMap : 암호화 확보 되지 않는다. - 운영중에 누구나 볼 수 있다.
  - Secret : 암호화 되어 있다.
  
--
PodSpec을 사용하여 Pod정의

apiVersion: v1
kind: Pod
metadata:
  name: pod-example

어떤 객체가 있을때, 어떤 API 정의가 Spec.을 정의하는 것이다.

k8s는 API의 set이다. 라고 할 수도 있다.

API : https://kubernetes.io/ko/docs/reference/using-api/

--
Volume
CSI: Container Storage Interface
  참고 블로그 글 : https://hyperconnect.github.io/2021/07/05/ebs-csi-gp3-support.html

참고: AWS는 EBS, EFS, 그리고 FSx for Lustre 가 있다.
Fargate의 경우 CSI 드라이버를 별도 구성할 필요가 없다. Fargate도 EFS를 지원한다.


pod eviction
resouce QoS
  - Best-Effort : 리소스 설정 안함. node 걸 다 그대로 씀
  - Bustable : limit - request 다른 값. 
  - Guaranteed : limit - request 같은 값.
  
  1순위로 죽이는것 Best-Effort
  2순위로 Bustable이 죽고,
  마지막으로 죽이는게 Guaranteed

--
resource 설정 관련해서
EKS가 아닌 private로 구성한 k8s cluster에서 control plane pod들을 찾아봤습니다.
예를 들어서, api-server 나 kube-proxy 같은 pod가 kube-system 이라는 namespace에 배포되어 있는데,
이 pod들은 제 생각에는 Best-Effort 면 안되고, Guaranteed 여야 한다고 생각했는데,
api-server와 kube-proxy pod가 resource가 정의되지 않은(Best-Effort) 형태로 설정되어 있는 것 같은데,
이건 제가 보고 있는 cluster가 올바르지 못한 설정을 했다고 봐야 할까요?

--
service의 selector 
pod의 label
참조관계에 사용된다. 

Service의 select에 type=dev가 설정되어 있고,
  Pod의 label에 type=dev가 설정되어 있으면, 
  
  Service의 select가 pod의 label을 참조한다고 보면 된다. 선택한다고 보면 된다.

Pod의 select는 Node의 label을 참조한다.

--
그리고 이제 taint와 tolerations 이 나온다.
node에 taint를 주면, 이 node에는 taint key value가 설정된 pod는 못 들어온다.
node에 tolerations 에 key value를 주면 이 tolerations을 갖는 pod가 들어올 수 있다.

--
DaemonSet

--
$ kubectl api-resources

--
이제 EKS

서로 다른 AZ에 배포하고 싶을때,
VPC의 subnet을 이용해서 AZ를 각기 다른 영역에 배포할 수 있다.

EKS의 여러 AZ에 pod을 배포하는것이 VPC의 subnet을 분리해서 AZ를 각기 다른 영역에 배포하는 것이라면,
VPC의 총 ip를 AZ에 각기 나눠줘야하는(나눠서 설정해 놓는) 것이고,
ip가 설정되지 않은 AZ는 사용할 수 없는(사용하지 않는) 것이라고 이해하면 되나요?
AZ를 구분하는 것이 VPC의 subnet이 유일한 확인방법(솔루션)인가 궁금해서요


데이터 플레인 옵션
1. EC2를 직접 구성하는 것. 하나 하나 직접 만드는 것
2. 관리형 노드그룹에 넣는 것. 이건 내가 만들지 않고 선언만 한다. 그럼 AWS가 만들어줌.
3. Fargate. 인프라에서 거의 손 떼는 것. AWS가 다 해달라고 하는 것.

우리 수업은 2번 방법
EKS에서는 Data Plane을 Node Group 이라고 부른다.

참고: Fargate는 아직 GPU 지원을 하지 않고 있다.

--
AWS
  - 인증: IAM - user role
  - 권한: IAM - policy
  
k8s
  - 인증: 별도 인증 프로바이더 제공하지 않음.
  - 권한: RBAC
*k8s는 인증 프로바이더를 별도로 제공하지 않는다.
 왜냐하면 대부분의 고객이 고객사의 인증시스템이 있기 때문에,
 굳이 k8s가 별도 인증 프로바이더를 제공하지 않는다.
 유저의 인증 프로바이더가 있을테니 그걸 사용하라는 의미.








