2021-10-06 AWS 강의 1일차

Kubernetes : 컨테이너 관리 도구
VM? App? Image?

왜 App은 컨테이너에 올려서 배포하는걸까? 목적이 뭘까?
  - 확장성
  - 관리용이성
  - 리소스(메모리) 관리
  - 마이크로 아키텍처 MSA

App을 컨테이너화 하는 대표적인 이유!
  - VM 보다 I/O에 대한 성능이 좋아짐?
  - 경량 VM이라고 볼 수 있다.
  
  Q1) 컨테이너가 On-Prem에 비해서 Kernel을 가지고 있지 않아서(공유해서) 경량이라는 말씀으로 이해해도 될까요?
  Q2) 경량이라는건 이해가 되는데, 경험적으로 I/O가 성능이 좋아진다는 경험은 못한 것 같은데,
      I/O 성능이 좋아지는 것이 어떤 예가 있을까요?
      App의 file, network i/o 성능이 On-Prem OS 상태일때 보다 좋아진다고 봐야 하나요?
      
  A) 아키텍처를 그려보면 보인다. 보통 On-Prem 위에 VM을 올려서 App을 배포하는데,
     이때는 On-Prem에도 kernel, VM에도 kernel이 있게 된다.
  
  
경량화 OS가 목적인 과제가 있는데, 관련해서 궁금한 점이 있습니다.
Q1) 경량화 OS가 목적이다 보니, Pod 하나 위에 컨테이너를 하나 올려서 서비스로 제공합니다.
    사실 유저에게 VM과 같은 환경을 컨테이너로 안정적으로 제공하고 싶었습니다.
    가장 처음 만난 문제가 Pod eviction 문제였습니다.
    간단하게 생각했을때, Pod eviction이 일어나지 않도록
    (1) Pod 리소스를 충분히 제공하는 것
    (2) eviction이 일어나지 않도록 configure를 수정하는 것을 검토했었습니다.
    이런 사례에서 정답이라고 할만한 방법이 없을까요?
    OS컨테이너가 좋지 못한 선택인가 싶어서요...
Q2) 경량화 OS(VM)을 목적으로 하는 과제가 현장에 사례가 많이 있나요?

A) VDI를 컨테이너로 가져가려는 시도들도 많이 있다.


네트워크 로직은 별도로 빼내서 만들고 관리하자. - 서비스 매시
엔터프라이즈 환경이 컨테이너들의 집합으로 커지면 결국 서비스 매시하고 연결될 수 밖에 없다.
이런 것들이 갈수록 생태계를 만들어 가고 있다.

k8s는 자동화 환경이 중요한 목적이기 때문에 GUI 콘솔 도구가 별로 없다.
CI/CD의 자동화가 큰 목적이다.

선언형
  - 개발에 있어서 코드 작성이 명령어 방식과 선언형 방식이 있는데, 어떤쪽이 자동화에 더 유리한가.
  - 선언형이 더 유리하다. yaml, json등을 사용하게 되는 배경.

k8s 내부 구조
  - 컨트롤 플레인 : 적절한 라우팅에 대한 결정 및 권한
  - 데이터 플레인 : 라우터를 통해 트래픽 전달

여러대의 호스트가 필요해지고 -> 클러스터 : 노드의 그룹

클러스터
  - 예전에는 master / slave
  - 최근에는 manager / worker 로 변경
  - 다른용어중에 control plane / data plane(또는 node cluster)
  
  Client -> req -> API Server -> Scheduler
    API Server : node 중에 어디에 배포할 것인가? 판단
                 이 판단을 하는 녀석 Scheduler

  이 요청을 받아서 처리하는 녀석이 kubelet
  kubelet이 일을 하고 결과에 문제가 없으면 -> API Server
  
  현상 : current
  사용자 요청 : desired (nginx를 2개 띄워라 같은 ...)
  
  Controller가 desired와 current가 맞는지(sync) 체크를 한다.
  
  그리고 데이터는 etcd에 저장
  --
  etcd, API Server, Scheduler, Controller 를 묶어서 control plane 이라고 부른다.
  
  --
  node cluster쪽은
  kubelet
  kube-proxy : 컨테이너의 통신을 책임진다
    - 최근에는 kube-proxy는 kernel 레벨의 iptables를 이용한다.
  
  --
  그리고 현업에서는 여기에 수 많은 plugin이 들어간다.
  
 --
 시스템이 커지면 위 등장인물들중 누가 워크로드가 가장 커질까?
 모든 통신은 API Server를 거쳐야 한다. 그래서 API Server가 워크로드가 가장 커진다.
 
 --
 EKS는 control plane은 보이지 않는다. 고객은 node cluster만 볼 수 있다.
 즉, EKS는 control plane을 관리해주는 관리형 서비스다.
 
 파게이트는(fargate)로 구성하면 Data plane도 보이지 않는다.
 이 경우는 Data plane도 EKS가 관리해 준다.
 
 --
 API Server는 Endpoint가 중요하다.
 API server는 2개의 Endpoint를 가지고 있다.
 하나는 내부를 위해서 하나는 외부와 연결하기 위해서.
 
 $ k cluster-info
Kubernetes master is running at https://8D3545D540B0C7E179498F5082A8BB1F.yl4.ap-northeast-2.eks.amazonaws.com
CoreDNS is running at https://8D3545D540B0C7E179498F5082A8BB1F.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://8D3545D540B0C7E179498F5082A8BB1F.yl4.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

여기 보이는 https 주소가 외부 endpoint

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
LGESCSICBLD37V[ /home/dongyoung.yoon/remove-cockpit-worker]$ k get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   172.20.0.1   <none>        443/TCP   223d
 
여기 보이는 ip가 내부 endpoint

-- 
Pod에 대해서
  - ReplicaSet이 Pod와 묶이면, Controller의 제어가 들어가게 됨
  - Controller 제어가 들어가면 Managed Pod
  - 그렇지 않은 Pod는 Unmanaged Pod
  - 보통 ReplicaSet을 직접 사용하지 않고, Deployment를 활용해서 ReplicaSet을 핸들링 한다.
  - Deployment > ReplicaSet > Pod 로 제어된다.
  - 보통 비동기 병렬처리로 job을 핸들링 한다고 보면 된다.
    수많은 App. 여러개를 병렬로 job을 실행한다고 보면 된다.
  - StatefulSet > Pod에 영향
  - Volume은 PVC, PV, Storage Class 등
  - HPA, VPA
  - ConfigMap : 암호화 확보 되지 않는다. - 운영중에 누구나 볼 수 있다.
  - Secret : 암호화 되어 있다.
  
--
PodSpec을 사용하여 Pod정의

apiVersion: v1
kind: Pod
metadata:
  name: pod-example

어떤 객체가 있을때, 어떤 API 정의가 Spec.을 정의하는 것이다.

k8s는 API의 set이다. 라고 할 수도 있다.

API : https://kubernetes.io/ko/docs/reference/using-api/

--
Volume
CSI: Container Storage Interface
  참고 블로그 글 : https://hyperconnect.github.io/2021/07/05/ebs-csi-gp3-support.html

참고: AWS는 EBS, EFS, 그리고 FSx for Lustre 가 있다.
Fargate의 경우 CSI 드라이버를 별도 구성할 필요가 없다. Fargate도 EFS를 지원한다.


pod eviction
resouce QoS
  - Best-Effort : 리소스 설정 안함. node 걸 다 그대로 씀
  - Bustable : limit - request 다른 값. 
  - Guaranteed : limit - request 같은 값.
  
  1순위로 죽이는것 Best-Effort
  2순위로 Bustable이 죽고,
  마지막으로 죽이는게 Guaranteed

--
resource 설정 관련해서
EKS가 아닌 private로 구성한 k8s cluster에서 control plane pod들을 찾아봤습니다.
예를 들어서, api-server 나 kube-proxy 같은 pod가 kube-system 이라는 namespace에 배포되어 있는데,
이 pod들은 제 생각에는 Best-Effort 면 안되고, Guaranteed 여야 한다고 생각했는데,
api-server와 kube-proxy pod가 resource가 정의되지 않은(Best-Effort) 형태로 설정되어 있는 것 같은데,
이건 제가 보고 있는 cluster가 올바르지 못한 설정을 했다고 봐야 할까요?

--
service의 selector 
pod의 label
참조관계에 사용된다. 

Service의 select에 type=dev가 설정되어 있고,
  Pod의 label에 type=dev가 설정되어 있으면, 
  
  Service의 select가 pod의 label을 참조한다고 보면 된다. 선택한다고 보면 된다.

Pod의 select는 Node의 label을 참조한다.

--
그리고 이제 taint와 tolerations 이 나온다.
node에 taint를 주면, 이 node에는 taint key value가 설정된 pod는 못 들어온다.
node에 tolerations 에 key value를 주면 이 tolerations을 갖는 pod가 들어올 수 있다.

--
DaemonSet

--
$ kubectl api-resources

--
이제 EKS

서로 다른 AZ에 배포하고 싶을때,
VPC의 subnet을 이용해서 AZ를 각기 다른 영역에 배포할 수 있다.

EKS의 여러 AZ에 pod을 배포하는것이 VPC의 subnet을 분리해서 AZ를 각기 다른 영역에 배포하는 것이라면,
VPC의 총 ip를 AZ에 각기 나눠줘야하는(나눠서 설정해 놓는) 것이고,
ip가 설정되지 않은 AZ는 사용할 수 없는(사용하지 않는) 것이라고 이해하면 되나요?
AZ를 구분하는 것이 VPC의 subnet이 유일한 확인방법(솔루션)인가 궁금해서요


데이터 플레인 옵션
1. EC2를 직접 구성하는 것. 하나 하나 직접 만드는 것
2. 관리형 노드그룹에 넣는 것. 이건 내가 만들지 않고 선언만 한다. 그럼 AWS가 만들어줌.
3. Fargate. 인프라에서 거의 손 떼는 것. AWS가 다 해달라고 하는 것.

우리 수업은 2번 방법
EKS에서는 Data Plane을 Node Group 이라고 부른다.

참고: Fargate는 아직 GPU 지원을 하지 않고 있다.

--
AWS
  - 인증: IAM - user role
  - 권한: IAM - policy
  
k8s
  - 인증: 별도 인증 프로바이더 제공하지 않음.
  - 권한: RBAC
*k8s는 인증 프로바이더를 별도로 제공하지 않는다.
 왜냐하면 대부분의 고객이 고객사의 인증시스템이 있기 때문에,
 굳이 k8s가 별도 인증 프로바이더를 제공하지 않는다.
 유저의 인증 프로바이더가 있을테니 그걸 사용하라는 의미.


--
2021-10-07 AWS 강의 2일차
  - 1일차 요약

  - 2일차 시작
  ### kubectl run --generator=run-pod/v1 resource-pod2 --image=busybox --restart Never --???

  Create Kubernetes federated cluster on AWS
  https://particule.io/en/blog/aws-federated-eks/
  : 여러개 리전으로 컨트롤 플레인 하나로 관리할 수 있도록 하는 것
    최근에 추가됨

  Operating a multi-regional stateless application using Amazon EKS
  https://aws.amazon.com/ko/blogs/containers/operating-a-multi-regional-stateless-application-using-amazon-eks/
  : 여러 리전 사용. 또다른 방법. 참고자료.

  k8s resource 단위 설명
  https://medium.com/swlh/understanding-kubernetes-resource-cpu-and-memory-units-30284b3cc866

  클러스터 배포 방법
  - AWS Console
  - AWS CLI : 가장 복잡하다. 어렵다.
  - eksctl : 권장하는 방법이다.

  AWS의 CloudFormation, cdk는 뭘까?

--
certificate-authority-data : 클러스터에 접속할 수 있는 인증서

cluster 배포 실습 : 동영상 - http://bit.ly/awstc-ekslabs

aws example github : https://github.com/weaveworks/eksctl/tree/main/examples

1번 실습 진행 : 클러스터 배포

--
Helm


--
GitOps
  - Jenkin-X, Flux, 아르고 ... 비슷한 도구

--
2번 실습
  - 두개의 파이프라인
  - 도커 파일 수정 > push > 빌드 > registry에 업로드 > flux가 이걸 인지 > heml > eks에 배포
  - 차트 파일 수정 > push > 빌드 > registry에 업로드 ... 동일

--
모듈 5
개요:
  - Amazon EKS 클러스터에서 관찰 기능 (observability) 구성
  - 지표 수집
  - 자동 스케일링에 지표 사용
  - 로그 관리
  - Amazon EKS에서 어플리케이션 추적
  - 관찰 기능으로부터 인사이트 얻기 및 적용
  - 실습 3


- k8s에서 지표는 네트워크상에 쌓아 놓아야 한다. 로컬에 쌓아 놓으면 의미가 없다.
- 지표는 중앙화 관리가 필요하다.

--
지표
  - Prometheus 지표 (지표 형식)
    : 우리가 생각하는 개념으로는 프로메테우스는 도구
      그것말고 프로메테우스의 개념은 지표의 형식. 즉 데이터의 형식에 대한 표준 포멧이다.
      그리고 추가로 PromQL 쿼리 언어가 있다. 그래서 프로메테우스를 쿼리 언어로 생각할 수도 있다.


EKS에서는
  - 컨트롤 플레인
    : 제대로 동작하는지 확인은 해야 하는것 아닌가?
      콘솔로 들어가면, EKS - Cluster - CloudWatch log group 에서 각 등장인물을 Enabled 시키면 된다.
      성능 데이터는 프로메테우스 포멧으로 생성이 되고 있다.
      이걸 알아낼 수 있는 방법은
       $ kubectl get --raw /metrics

    * 참고로 최근에 EKS에는 Serverless로
    Amazon Managed Service for Prometheus (아직 서울 리전에 적용은 안됐음)
    Amazon Managed Service for Grafana
    가 생겼다.
    
    Prometheus 또는 Grafana 가 대세 도구가 되기전에 EKS에는 CloudWatch - Container Insights 가 있었다. (지금도 있다)

  - 데이터 플레인
    : 

  - VPA(Vertical Pod Autoscaler) 는 별도 설치


-
추적(Tracing)


----
3일차
9:00부터 바로 실습 3번 시작하면 됨

logging & tracing

--
Pod 중단 예산 (Pod Disruption Budgets)

kind: PodDisruptionBudget
metadata:
    name: webapp-pdb
spec:
    minAvailable: 2

위 상황이라면, 아래 명령중 2번은 실패하게 된다.

1) kubectl drain node1
2) kubectl drain node2

--
오버프로비저닝을 통한 확장 가속화

kind: PriorityClass

누군가 다른 pod를 쫓아내고, 내가 들어간다.
이걸 누구를 쫓아낼건가 하는건, Priority로 정해진다.

이때 쫓겨난 Pod는 새로운 노드가 만들어지고, 여기 들어가게 된다.
그래서 이렇게 쫓겨나는 Pod는 사실 아무일도 안하는 Pod인데,
이런 용도 (빠르게 노드를 프로비저닝할 목적)으로 활용하는 Pod라고 보면 된다.

--
Q1) 적절한 노드 사이즈, 스팟 인스턴스 등을 적용한다고 하고도,

운영을 하다 보니 언제든 쫓겨나도 괜찮은 Pod만 배포된 Node가 종종 생기기도 하는 것 같습니다.
(물론 이런 스케쥴이 일어나면 안되겠지만...)
어쩌다 보니 이런 Pod만 배포된 Node가 몇개 생겼다면...

이런 Node를 드레인 하는게 비용 절감 때문에 필요한 상황을 만나고 있는데,
이런 종류의 모니터링(드레인 할 노드를 찾기)은 Cronjob에서 계속 찾아봐야 할까요?
낭비되는 노드를 찾아내서 드레인 하는 (자동화된) 좋은 방법이 있을까요?


Q2) 꽤 고사양(64core 이상)의 Node가 필요한데 몇시간(4-5시간?) 정도 사용하고 Pod이 종료되면 됩니다.
이런 경우가 있는데, 지금은 사용이 끝나면 노드를 드레인 하는 방법으로 사용하고 있습니다.
fargate는 좋은 선택은 되지 못할까요?
사용이 끝나고 Drain하거나 하는 부분을 더 고민(관리)하고 싶지 않아서 다른 방법이 있을지 계속 생각해 보고 있습니다.

--
비용 최적화 개요

Firecracker 가 Fargate다. EC2가 아니다.
microVM이다.


--
모듈7 : 네트워킹 관리
중요!!!!!

SDN : Software Defined Network
이게 이제 서비스매시로 변화

지금은 proxy가 천하통일함. 이 프록시가 envoy
envoy가 7 layer를 관리

--
EC2의 network interface : eni

--
컨테이너간 통신은 동일 pod안에서 가능
서로 다른 pod안에 있는 컨테이너는 서로 통신 못함

pod간 통신은
  - 



--
서비스는 cluster-ip가 있다.


노드 하나에 pod이 여러개 떠 있고, pod들은 각각 한개의 컨테이너를 가지고 있습니다.
컨테이너가 sshd 를 열어둔 상태입니다.

외부에서 이 동일노드에 있는 pod들에 ssh로 접속을 하려고 하는데,
이때 설정할 수 있는 방법이 nodeport를 열어서, 각 pod의 ssh를 찾아가는걸 포트번호로 나눴습니다.

nodeport 가 닫혀버리면, 외부에서 pod에 ssh로 접속하지 못하고요.

ssh의 경우 7계층에서 나눌수가 없어서...






<비공개 대화입니다>
<이 질문은 전체 공개가 되지 않으면 좋겠습니다.>
<간단하게 답변 주실 수 있다면 채팅 메시지로 의견 주실 수 있을까요?>

강의가 끝날쯤에 쉬는시간에 강사님께 여쭤보려고 했던 겁니다.

제가 참여하는 과제는 k8s로 ubuntu를 container로 만들어서 container마다 회사내 개발자(사용자)에게 ssh로 접속해서
이 container를 일종의 VM처럼 개발자들이 사용하게 하는 과제 입니다.

그래서 수업중 2가지 질문을 드렸었습니다.
1) 64core 정도 되는 대용량 머신을 fargate로 제공하는 것에 대해서...
2) nodeport를 만들어서 포트 번호로 구분해서 pod들에 ssh 접속을 하는 것...

1)번은 회사내 개발자가 ssh로 container에 접속해서 결국 하는 일은 대규모 SW 예를 들면 안드로이드 같은 프로젝트의
Full Compile을 하기 때문에 CPU 가용자원이 큰 노드가 필요했습니다. compile이 끝나면 노드를 회수하고 싶었던 것이고요.
이런 모니터링을 configmap으로 대략 10~30분에 한번씩 무언가를 살펴 보는 스크립트로 하고 있는데, 좀 복잡한 것 같아서 fargate를 여쭤봤었고

2)번 역시 개발자(유저)가 자신의 container에 ssh로 접속할 수 있는 유니크한 주소가 나와야 해서,
ssh니까 ALB로 올리지 못하고, ingresss를 사용하지 못해서 nodeport를 이용한 접속을 하는데 다른 방법이 있을까 한번 생각해 봤던 거였습니다.

여기까지는 제가 참여하는 과제에 대한 백그라운드고,
여쭙고 싶은건,

혹시 외부에 나가보면, 개발팀에서 사용할 개발환경 자체를 k8s를 이용해서 container로 나눠서
사내 개발시스템을 구축하는 그런 프로젝트를 들어보신적이 있는지 좀 궁금합니다.

k8s가 앱이나 서비스 레벨에서는 당연히 잘 맞는 솔루션인데,
VM처럼 하나씩 사내 사용자들에게 나눠주는 것도 k8s를 사용하는게 철학적으로(?) 맞나 싶은 생각이 가끔 들어서요.

혹시 이런 접근을 들어보신 적이 있으신지? 갑자기 여쭤서 죄송하긴 한데, 강사님 의견은 어떠신지 짧게라도 들어보고 싶습니다.




--
인증
  - serviceAccountName




--
calico를 통한 네트워크 통신 보안
보안 정책을 제공하기 때문에 많이 사용
eks에서는 cni를 사용함. 하지만 cni에도 보안정책은 없음.
그래서 eks에서도 보안정책은 calico를 recommand하고 있음.




















